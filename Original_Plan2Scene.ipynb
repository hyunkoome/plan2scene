{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeuGlqwL-pno"
   },
   "source": [
    "# Plan2Scene: Converting Floorplans to 3D Scenes\n",
    "Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel X. Chang , Manolis Savva\n",
    "\n",
    "Simon Fraser University\n",
    "\n",
    "*CVPR 2021*\n",
    "\n",
    "\n",
    "[[Project Page](https://3dlg-hcvc.github.io/plan2scene/), [GitHub](https://github.com/3dlg-hcvc/plan2scene), [Paper](https://arxiv.org/abs/2106.05375)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNpcgVKPeP5k"
   },
   "source": [
    "<img src='https://raw.githubusercontent.com/3dlg-hcvc/plan2scene/main/docs/img/intro.png' height='300'/>\n",
    "\n",
    "Our system addresses the Plan2Scene task by converting a floorplan and set of photos into a textured 3D mesh model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYoCurNIfOyU"
   },
   "source": [
    "## Plan2Scene Stages\n",
    "<img src='https://raw.githubusercontent.com/3dlg-hcvc/plan2scene/main/docs/img/task-overview.png'/>\n",
    "\n",
    "The Plan2Scene task consists of several steps: floorplan vectorization, 3D geometry construction, object placement, photo assignment, texture generation, and texture propagation.\n",
    "\n",
    "This notebook focuses on the texture generation and the texture propagation steps of the task. We assume a vectorized floorplan is available, and the photos are assigned to rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-dQxv2fbjxW"
   },
   "source": [
    "## Pre-requisites\n",
    "1. __Run this notebook using a GPU runtime.__\n",
    "\n",
    "2. You are required to upload a **floorplan vector of a house** (in scene.json format), **extracted rectified surface crops**, and **photo room assignments** to use this notebook. You can refer to our dataset [Rent3D++](https://3dlg-hcvc.github.io/plan2scene/) for the above information.\n",
    "\n",
    "3. To preview the final textured 3D house, you have to use [smart scene toolkit](https://github.com/smartscenes/sstk) scene viewer in your local machine. This notebook allows previewing of synthesized textures as individual images, but not as a 3D model."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!nvcc -V"
   ],
   "metadata": {
    "id": "kDB27kf5nX98",
    "outputId": "cc79e5dc-a112-4899-a5d9-343778b1e9f2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypXzvR55Qtqq",
    "outputId": "5c237589-5151-4b41-de2e-d1cbea75ab0f"
   },
   "source": [
    "# Download the code\n",
    "!git clone https://github.com/3dlg-hcvc/plan2scene.git"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'plan2scene' already exists and is not an empty directory.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBFdj-swSgcV"
   },
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGP3ALWtOAGT",
    "outputId": "68ff04c3-2fc5-4ffd-dd5e-a5bddc0ce9cc"
   },
   "source": [
    "# Install torch\n",
    "# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m84.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m55.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m90.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m6.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m22.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m73.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuBbRDNgS3LO",
    "outputId": "67cadac4-fa1f-4a2e-d130-e83b043330a2"
   },
   "source": [
    "# Install requirements\n",
    "!pip install numpy pandas pyyaml orderedattrdict kornia>=0.2.0 tensorboard git+https://github.com/sbrisard/moisan2011.git@master pytorch-fid>=0.1.1"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/sbrisard/moisan2011.git /tmp/pip-req-build-qro04muq\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVGdKvoROBZo",
    "outputId": "9acfa9a4-e1bb-45e2-bafb-b5c304d56961"
   },
   "source": [
    "# Install PyTorch Geometric\n",
    "# %env CUDA=cu110\n",
    "%env CUDA=cu12.4\n",
    "# %env TORCH=1.7.1\n",
    "%env TORCH=2.6.0\n",
    "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --no-cache\n",
    "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --no-cache # This step takes unusually long. About 30 minutes.\n",
    "# !pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --no-cache\n",
    "# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --no-cache\n",
    "!pip install torch-geometric --no-cache"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: CUDA=cu12.4\n",
      "env: TORCH=2.6.0\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu12.4.html\n",
      "Collecting torch-scatter\n",
      "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m108.0/108.0 kB\u001B[0m \u001B[31m15.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Building wheels for collected packages: torch-scatter\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3622721 sha256=b9c7db8d284744ffddd8682597ea39274a87d5551672d0cd8100c6bbea814bbf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ybgeways/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
      "Successfully built torch-scatter\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.2\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu12.4.html\n",
      "Collecting torch-sparse\n",
      "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m210.0/210.0 kB\u001B[0m \u001B[31m19.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
      "Building wheels for collected packages: torch-sparse\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=2846205 sha256=da3774942b86d3f2bb13cb327cdf8b608f1a38a91b38b3ef47be22cc2f088c3a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xxfv8_6v/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\n",
      "Successfully built torch-sparse\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.6.18\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.1/63.1 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m66.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.6.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TznP7y1lQxkP"
   },
   "source": [
    "# Install noise kernel\n",
    "%cd ./plan2scene\n",
    "%cd code/src/plan2scene/texture_gen/custom_ops/noise_kernel\n",
    "!pip install .\n",
    "%cd ../../../../../../../"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g699pK8thDt-"
   },
   "source": [
    "# Download EmbarkStudios Texture Synthesis library. We use it to correct seams of textures so they can be tiled.\n",
    "%env EMBARK_TEX_SYNTH=texture-synthesis-0.8.2-x86_64-unknown-linux-musl\n",
    "!wget https://github.com/EmbarkStudios/texture-synthesis/releases/download/0.8.2/$EMBARK_TEX_SYNTH.tar.gz\n",
    "!tar -xf $EMBARK_TEX_SYNTH.tar.gz\n",
    "\n",
    "# Download seam mask\n",
    "!wget -P $EMBARK_TEX_SYNTH https://raw.githubusercontent.com/EmbarkStudios/texture-synthesis/main/imgs/masks/1_tile.jpg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qwdHxqB8hynP"
   },
   "source": [
    "# Specify seam correction configuration\n",
    "import os\n",
    "import os.path as osp\n",
    "import json\n",
    "texture_synthesis_conf = {\n",
    "    \"texture_synthesis_path\": osp.abspath(osp.join(os.environ[\"EMBARK_TEX_SYNTH\"], \"texture-synthesis\")),\n",
    "    \"seam_mask_path\": osp.abspath(osp.join(os.environ[\"EMBARK_TEX_SYNTH\"], \"1_tile.jpg\"))\n",
    "}\n",
    "with open(\"./plan2scene/conf/plan2scene/seam_correct.json\", \"w\") as f:\n",
    "  json.dump(texture_synthesis_conf, f, indent=4)\n",
    "\n",
    "assert osp.exists(texture_synthesis_conf[\"texture_synthesis_path\"])\n",
    "assert osp.exists(texture_synthesis_conf[\"seam_mask_path\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnRdKOBjZTpY"
   },
   "source": [
    "## Load Plan2Scene Modules\n",
    "__Re-run from here if you restart the runtime.__"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HR2VVWhUIluM"
   },
   "source": [
    "%cd ./plan2scene"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fgRuHJJpYOKT",
    "ExecuteTime": {
     "end_time": "2025-06-18T01:47:44.079142Z",
     "start_time": "2025-06-18T01:47:42.373347Z"
    }
   },
   "source": [
    "# Load system modules\n",
    "import zipfile\n",
    "import io\n",
    "import os.path as osp\n",
    "import os\n",
    "# from google.colab import files\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact\n",
    "from PIL import ImageDraw, Image\n",
    "import torch\n",
    "\n",
    "# Load Plan2Scene modules\n",
    "sys.path.append(\"./code/src\")\n",
    "from arch_parser.parser import parse_arch_json, parse_object_jsons, PreferredFormat, parse_scene_json_from_file\n",
    "from plan2scene.common.residence import House\n",
    "from plan2scene.config_manager import ConfigManager\n",
    "from plan2scene.common.image_description import ImageDescription, ImageSource\n",
    "from plan2scene.utils.io import load_image\n",
    "from arch_parser.json_util import pil_to_data_url\n",
    "from plan2scene.texture_gen.predictor import TextureGenPredictor\n",
    "from plan2scene.texture_gen.utils.io import load_conf_eval\n",
    "from plan2scene.crop_select.util import fill_textures\n",
    "from plan2scene.texture_prop.utils import update_embeddings\n",
    "from plan2scene.texture_prop.predictor import TexturePropPredictor\n",
    "from plan2scene.texture_prop.graph_generators import InferenceHGG\n",
    "from plan2scene.texture_prop.houses_dataset import HouseDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from plan2scene.common.house_parser import save_arch\n",
    "from plan2scene.utils.tile_util import tile_image\n",
    "\n",
    "# Initialize config manager\n",
    "conf = ConfigManager()\n",
    "conf.load_default_args()\n",
    "conf.texture_gen.texture_synth_conf = \"./conf/plan2scene/texture_synth_conf/v2.yml\""
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using seed: 12415\n",
      "INFO:root:Args: Namespace(seed=12415, data_paths='./conf/plan2scene/data_paths.json', house_gen='./conf/plan2scene/house_gen.json', metric_conf='./conf/plan2scene/metric.json', texture_gen='./conf/plan2scene/texture_gen.json', texture_prop='./conf/plan2scene/texture_prop_conf/default.json', render_config='./conf/plan2scene/render.json', seam_correct_config='./conf/plan2scene/seam_correct.json', labels_path='./conf/plan2scene/labels', log_level='INFO', drop='0.0', num_workers=4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ytcANMEDbX0s"
   },
   "source": [
    "def upload():\n",
    "  \"\"\"\n",
    "  Utility method for file uploads.\n",
    "  \"\"\"\n",
    "  uploaded = files.upload()\n",
    "  assert len(uploaded) == 1\n",
    "  content = None\n",
    "  for key in uploaded.keys():\n",
    "    print('Uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "        name=key, length=len(uploaded[key])))\n",
    "    return uploaded[key]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_json_as_str(path, encoding=\"utf-8\"):\n",
    "    \"\"\"\n",
    "    Read a JSON file from disk and return its contents as a string.\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    return data.decode(encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T02:05:07.490993Z",
     "start_time": "2025-06-18T02:05:07.488999Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiA4i6MsX6Bw"
   },
   "source": [
    "## Data\n",
    "In this section, we will obtain the following files from you.\n",
    "1. A floorplan vector in the scene.json format. Refer to the Rent3D++ dataset for examples. The scene.json format is [explained here](https://github.com/3dlg-hcvc/plan2scene/blob/main/docs/md/scene_json_format.md).\n",
    "2. Rectified surface crops extracted from photos of the house.\n",
    "  - If you wish to extract new rectified surface crops, [refer to instructions here](https://github.com/3dlg-hcvc/plan2scene/blob/main/docs/md/extract_crops.md).\n",
    "  - Otherwise, you can provide rectified surface crops provided with the Rent3D++ dataset.\n",
    "3. Photo room assignments specified in a photoroom.csv file. Refer to the Rent3D++ dataset for examples. The format of the photoroom.csv file is [described here](https://github.com/3dlg-hcvc/plan2scene/blob/main/docs/md/rent3dpp_data_organization.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLPwp19DYEOq"
   },
   "source": [
    "__Task:__ Upload a scene.json file describing a floorplan vector."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M2ERdfn6SVjy",
    "ExecuteTime": {
     "end_time": "2025-06-18T02:07:17.131467Z",
     "start_time": "2025-06-18T02:07:17.120177Z"
    }
   },
   "source": [
    "house_key = None\n",
    "if not osp.exists(\"./data/processed/archs\"):\n",
    "  os.makedirs(\"./data/processed/archs\")\n",
    "\n",
    "# scene_json_content = upload()\n",
    "scene_json_path = \"/home/hyunkoo/DATA/HDD8TB/Project/plan2scene/data/processed/archs_with_hole_models/test/28025487.scene.json\"\n",
    "# scene_json = json.loads(scene_json_content.decode())\n",
    "scene_json_content_decode = load_json_as_str(scene_json_path)\n",
    "# scene_json = json.loads(scene_json_content.decode())\n",
    "scene_json = json.loads(scene_json_content_decode)\n",
    "scene_id = scene_json[\"scene\"][\"arch\"][\"id\"]\n",
    "\n",
    "with open(f\"./data/processed/archs/{scene_id}.scene.json\", \"w\") as f:\n",
    "  # f.write(scene_json_content.decode())\n",
    "  f.write(scene_json_content_decode)\n",
    "\n",
    "print(\"Sketch of floorplan vector.\")\n",
    "arch_house = parse_scene_json_from_file(f\"./data/processed/archs/{scene_id}.scene.json\",  None)\n",
    "arch_house.sketch_house()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sketch of floorplan vector.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=400x400>",
      "image/png": "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"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57Dp_CcMeaMx"
   },
   "source": [
    "__Task:__ Upload rectified surface crops extracted from photos.\n",
    "\n",
    "The rectified surface crops should be named in the format '[photo_file_name_without_extension]_[surface_id]_crop[crop_id].png'. The rectified surface crops provided with the Rent3D++ dataset are in this format. If you extract new rectified surface crops using the code we provide, they will also be in this format.\n",
    "\n",
    "When you run the next cell, you will be prompted to upload 3 zip files, one per surface type, containing crops sampled from rectified surfaces.\n",
    "\n",
    "For the Rent3D++ dataset, you can create such zip files using the rectified crops available in the './processed/rectified_crops/[surface_type]' directories provided by the Rent3D++ dataset. Note that each photo of the Rent3D++ dataset has a filename starting with the house_key. So, you can search and identify the crops belonging to the scene.json file you uploaded before."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SU2-nxcNY9xw"
   },
   "source": [
    "for surface in conf.surfaces:\n",
    "  print(f\"Please provide a .zip file containing rectified crops extracted from {surface} surfaces.\")\n",
    "\n",
    "  if not osp.exists(f\"./data/processed/rectified_crops/{surface}\"):\n",
    "    os.makedirs(f\"./data/processed/rectified_crops/{surface}\")\n",
    "\n",
    "  crops_content = upload()\n",
    "  zf = zipfile.ZipFile(io.BytesIO(crops_content), \"r\")\n",
    "  zf.extractall(path=f\"./data/processed/rectified_crops/{surface}\")\n",
    "  print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2eo8kbdigi3"
   },
   "source": [
    "__Task:__ Upload a photoroom.csv file describing the assignment of photos to rooms."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u_G2bZwsf_dJ"
   },
   "source": [
    "photoroom_content = upload()\n",
    "\n",
    "os.makedirs(\"./data/processed/photo_assignments\", exist_ok=True)\n",
    "\n",
    "photo_room_df = pd.read_csv(io.BytesIO(photoroom_content))\n",
    "assert \"roomId\" in photo_room_df.columns\n",
    "assert \"photo\" in photo_room_df.columns\n",
    "photo_room_df.to_csv(f\"./data/processed/photo_assignments/{scene_id}.photoroom.csv\")\n",
    "photo_room_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4GveKRnugfDY"
   },
   "source": [
    "# Re-load house with crop assignments\n",
    "arch_house = parse_scene_json_from_file(f\"./data/processed/archs/{scene_id}.scene.json\", f\"./data/processed/photo_assignments/{scene_id}.photoroom.csv\")\n",
    "house = House.from_arch_house(arch_house, surfaces=conf.surfaces)\n",
    "room_count = len(house.rooms)\n",
    "\n",
    "for room_index, room in house.rooms.items():\n",
    "    for photo in room.photos:\n",
    "        for surface in conf.surfaces:\n",
    "            surface_instances = [i for i in range(conf.texture_gen.masks_per_surface[surface])]\n",
    "            for surface_instance in surface_instances:\n",
    "                for crop_instance in range(conf.texture_gen.crops_per_mask):\n",
    "                    candidate_key = \"%s_%d_crop%d\" % (photo, surface_instance, crop_instance)\n",
    "                    if osp.exists(osp.join(conf.data_paths.rectified_crops_path, surface, candidate_key + \".png\")):\n",
    "                        image = load_image(\n",
    "                            osp.join(conf.data_paths.rectified_crops_path, surface, candidate_key + \".png\"))\n",
    "                        room.surface_textures[surface][candidate_key] = ImageDescription(image, ImageSource.NEURAL_SYNTH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5kgQYqlWb8P"
   },
   "source": [
    "__Task:__ Let's preview the data you have provided.\n",
    "\n",
    "Run the below cell. Then select a room_id from the dropbox to preview crops assigned to a room."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qUuitcX5WfPN"
   },
   "source": [
    "room_ids = list(house.rooms)\n",
    "\n",
    "@interact(room_id=room_ids)\n",
    "def preview_room_crops(room_id):\n",
    "  room = house.rooms[room_id]\n",
    "\n",
    "  html_buffer = io.StringIO()\n",
    "  html_buffer.write(\"<table>\\n\")\n",
    "\n",
    "  for i, surface in enumerate(conf.surfaces):\n",
    "    html_buffer.write(\"<tr>\\n\")\n",
    "    if i == 0:\n",
    "      # Left section\n",
    "      html_buffer.write(\"<td rowspan='{colspan}'>\\n\".format( colspan=len(conf.surfaces)))\n",
    "      html_buffer.write(\"<img src='{src}'/>\\n\".format(src=pil_to_data_url(house.sketch_house(focused_room_id=room.room_id))))\n",
    "      html_buffer.write(\"<br/>\\n\")\n",
    "      html_buffer.write(\"<p style='text-align:center;'><b>Room Types: </b>{types}</p>\\n\".format(types=str(room.types)))\n",
    "      html_buffer.write(\"</td>\\n\")\n",
    "\n",
    "    # Right section\n",
    "    html_buffer.write(\"<td>\\n\")\n",
    "    html_buffer.write(f\"<span>{surface} crops</span>\\n\")\n",
    "    html_buffer.write(\"<br/>\\n\")\n",
    "    for crop_id, crop in room.surface_textures[surface].items():\n",
    "      html_buffer.write(\"<img src='{src}'/>\\n\".format(src=pil_to_data_url(crop.image)))\n",
    "    html_buffer.write(\"</td>\\n\")\n",
    "    html_buffer.write(\"</tr>\\n\")\n",
    "\n",
    "  html_buffer.write(\"</table>\\n\")\n",
    "  display(HTML(html_buffer.getvalue()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54deSSG3oozH"
   },
   "source": [
    "# Synthesize textures for observed surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkrA3eP4o7yW"
   },
   "source": [
    "Download pre-trained weights for the texture synthesis stage. We use the pre-trained model trained on stationary textures dataset version 2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o9J02fqzgQlo"
   },
   "source": [
    "%env SYNTH_CHECKPOINT_NAME=loss-7.67493-epoch-750.ckpt\n",
    "!mkdir ./trained_models/texture_gen/v2/checkpoints\n",
    "!wget -P ./trained_models/texture_gen/v2/checkpoints http://aspis.cmpt.sfu.ca/projects/plan2scene/pretrained-models/texture-synth/v2/checkpoints/$SYNTH_CHECKPOINT_NAME\n",
    "conf.texture_gen.checkpoint_path = \"./trained_models/texture_gen/v2/checkpoints/{checkpoint_name}\".format(checkpoint_name = os.environ[\"SYNTH_CHECKPOINT_NAME\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yRXSfiqOgQ-w"
   },
   "source": [
    "# Load texture synthesis network\n",
    "tg_predictor = TextureGenPredictor(\n",
    "        conf=load_conf_eval(config_path=conf.texture_gen.texture_synth_conf),\n",
    "        rgb_median_emb=conf.texture_gen.rgb_median_emb)\n",
    "tg_predictor.load_checkpoint(checkpoint_path=conf.texture_gen.checkpoint_path)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YOhn9OGxqB2P"
   },
   "source": [
    "# Compute texture embeddings for observed surfaces (Code adapted from ./code/scripts/preprocessing/fill_room_embeddigs.py)\n",
    "for room_index, room in house.rooms.items():\n",
    "    for surface in conf.surfaces:\n",
    "      for candidate_key, image_description in room.surface_textures[surface].items():\n",
    "        image = image_description.image\n",
    "        emb, loss = tg_predictor.predict_embs([image])\n",
    "        room.surface_embeddings[surface][candidate_key] = emb\n",
    "        room.surface_losses[surface][candidate_key] = loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OEby3IPQrdV7"
   },
   "source": [
    "# Synthesize textures for observed surfaces using the most suitable crop identified by VGG textureness score.\n",
    "# Code copied from ./code/scripts/crop_select/vgg_crop_selector.py)\n",
    "\n",
    "def get_least_key(kv):\n",
    "    \"\"\"\n",
    "    Given a dictionary, returns the key with minimum value.\n",
    "    :param kv: Dictionary considered.\n",
    "    :return: Key with the minimum value.\n",
    "    \"\"\"\n",
    "    min_k = None\n",
    "    min_v = None\n",
    "    for k, v in kv.items():\n",
    "        if min_v is None or v.item() < min_v:\n",
    "            min_k = k\n",
    "            min_v = v.item()\n",
    "\n",
    "    return min_k\n",
    "\n",
    "for room_index, room in house.rooms.items():\n",
    "    # Calculate the least VGG loss embeddings\n",
    "    for surface in room.surface_embeddings:\n",
    "        least_key = get_least_key(room.surface_losses[surface])\n",
    "        if least_key is not None:\n",
    "            room.surface_embeddings[surface] = {\"prop\": room.surface_embeddings[surface][least_key]}\n",
    "            room.surface_losses[surface] = {\"prop\": room.surface_losses[surface][least_key]}\n",
    "        else:\n",
    "            room.surface_embeddings[surface] = {}\n",
    "            room.surface_losses[surface] = {}\n",
    "\n",
    "fill_textures(conf, {house.house_key: house}, predictor=tg_predictor, log=False, image_source=ImageSource.VGG_CROP_SELECT, skip_existing_textures=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiDoz5LvukW3"
   },
   "source": [
    "# Propagate Textures to Unobserved Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OEmIMXCFuYoG"
   },
   "source": [
    "# Download pre-trained weights for the GNN.\n",
    "%env PROP_CHECKPOINT_NAME=loss-0.51442-epoch-250.ckpt\n",
    "!mkdir ./trained_models/texture_prop/v2/checkpoints\n",
    "!wget -P ./trained_models/texture_prop/v2/checkpoints https://aspis.cmpt.sfu.ca/projects/plan2scene/pretrained-models/texture-prop/synth-v2-epoch750/checkpoints/$PROP_CHECKPOINT_NAME\n",
    "\n",
    "prop_checkpoint_path = \"./trained_models/texture_prop/v2/checkpoints/{checkpoint_name}\".format(checkpoint_name = os.environ[\"PROP_CHECKPOINT_NAME\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VWDb8cYbvkcA"
   },
   "source": [
    "# Load GNN and graph generator\n",
    "# Code copied from ./code/scripts/texture_prop/gnn_texture_prop.py\n",
    "tp_predictor = TexturePropPredictor(conf, conf.texture_prop)\n",
    "tp_predictor.load_checkpoint(checkpoint_path=prop_checkpoint_path)\n",
    "nt_graph_generator = InferenceHGG(conf=conf, include_target=False)\n",
    "\n",
    "# Graph dataset\n",
    "houses = {scene_id: house}\n",
    "\n",
    "nt_dataset = HouseDataset(houses, graph_generator=nt_graph_generator)\n",
    "nt_dataloader = DataLoader(nt_dataset, batch_size=conf.texture_prop.train.bs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s0IW4WIDybFv"
   },
   "source": [
    "# Graph inference\n",
    "# Code copied from ./code/scripts/texture_prop/gnn_texture_prop.py\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(nt_dataloader):\n",
    "        print(\"Batch [%d/%d] Graph Inference\" % (i, len(nt_dataloader)))\n",
    "        output = tp_predictor.predict(batch.to(conf.texture_prop.device))\n",
    "        update_embeddings(conf, houses, batch, output,\n",
    "                          keep_existing_predictions=True)\n",
    "\n",
    "fill_textures(conf, houses, log=True, predictor=tg_predictor, image_source=ImageSource.GNN_PROP, skip_existing_textures=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nhfa8_5Gz6c4"
   },
   "source": [
    "# Correct seams of synthesized textures\n",
    "We correct seams of synthesized textures, making them tileable textures."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HO0ROxQ6jz1x"
   },
   "source": [
    "embark_texture_synthesis_path = conf.seam_correct_config.texture_synthesis_path\n",
    "seam_mask_path = conf.seam_correct_config.seam_mask_path\n",
    "\n",
    "for room_index, room in house.rooms.items():\n",
    "  print(f\"Processing {room_index}/{room_count}\")\n",
    "  for surface in room.surface_textures:\n",
    "      texture_description = room.surface_textures[surface][\"prop\"]\n",
    "      assert isinstance(texture_description, ImageDescription)\n",
    "      texture = texture_description.image\n",
    "      texture = tile_image(texture, embark_texture_synthesis_path, seam_mask_path)\n",
    "      texture_description.image = texture"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CyT4nzd2JD3"
   },
   "source": [
    "## Preview Synthesized Textures\n",
    "\n",
    "Run the below cell to preview the texture crops synthesized for each surface. Use the drop-down to select a room.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d9llYb4Y3Ouw"
   },
   "source": [
    "@interact(room_id=room_ids)\n",
    "def preview_room_textures(room_id):\n",
    "  room = house.rooms[room_id]\n",
    "\n",
    "  html_buffer = io.StringIO()\n",
    "  html_buffer.write(\"<table>\\n\")\n",
    "\n",
    "  for i, surface in enumerate(conf.surfaces):\n",
    "    html_buffer.write(\"<tr>\\n\")\n",
    "    if i == 0:\n",
    "      # Left section\n",
    "      html_buffer.write(\"<td rowspan='{colspan}'>\\n\".format( colspan=len(conf.surfaces)))\n",
    "      html_buffer.write(\"<img src='{src}'/>\\n\".format(src=pil_to_data_url(house.sketch_house(focused_room_id=room.room_id))))\n",
    "      html_buffer.write(\"<br/>\\n\")\n",
    "      html_buffer.write(\"<p style='text-align:center;'><b>{types}</b></p>\\n\".format(types=str(room.types)))\n",
    "      html_buffer.write(\"</td>\\n\")\n",
    "\n",
    "    # Right section\n",
    "    html_buffer.write(\"<td><span>{surface}</span><br/><img src='{src}'/></td>\\n\".format(src=pil_to_data_url(room.surface_textures[surface][\"prop\"].image), surface=surface))\n",
    "    html_buffer.write(\"</tr>\\n\")\n",
    "\n",
    "  html_buffer.write(\"</table>\\n\")\n",
    "  display(HTML(html_buffer.getvalue()))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-47WCtin0E9c"
   },
   "source": [
    "## Output Model of House\n",
    "You can download the textured scene.json file by running the cell below. Open it using the [smart scene toolkit](https://github.com/smartscenes/sstk) scene viewer to view the 3D model of the house. You have to setup the scene viewer in your local machine to preview it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SFL7Qdvc0Bqb"
   },
   "source": [
    "if not osp.exists(\"./outputs\"):\n",
    "  os.makedirs(\"./outputs\")\n",
    "save_arch(conf, house, osp.join(\"./outputs/\", scene_id), texture_both_sides_of_walls = True)\n",
    "\n",
    "from google.colab import files\n",
    "files.download(osp.join(\"./outputs/\", scene_id + \".scene.json\"))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
